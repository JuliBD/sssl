{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs01/berens/user/jdoehl/GitHub/sssl\n"
     ]
    }
   ],
   "source": [
    "# change directory to sssl folder\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo:  \n",
    "- Use an embedding from a embedding layer for each datapoint (50k to mimic cifar10 train)\n",
    "- Add some noise on the embeddings before computing loss and gradients (to mimic transformations in SimCLR)\n",
    "- Maybe: Use a cos_sim matrix (50k x 50k) from a fully SimCLR model trained for 1k epochs on cifar10 as the target similarities or something like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5881, 21788, 40530,  8727]),\n",
       " tensor([[1.0000, 0.3835, 0.3143, 0.4438],\n",
       "         [0.3835, 1.0000, 0.6289, 0.1714],\n",
       "         [0.3143, 0.6289, 1.0000, 0.4139],\n",
       "         [0.4438, 0.1714, 0.4139, 1.0000]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "class CosSimModel(torch.nn.Module):\n",
    "    def __init__(self, dataset_size):\n",
    "        super().__init__()\n",
    "        self.cos_sim_matrix = torch.nn.Parameter(torch.diag(torch.ones(dataset_size)))\n",
    "    def forward(self, ids):\n",
    "        return self.cos_sim_matrix[ids][:,ids]\n",
    "\n",
    "dataset_size = 50000\n",
    "cos_sim_model = CosSimModel(dataset_size)\n",
    "optimizer = torch.optim.Adam(cos_sim_model.parameters(), lr=0.01)\n",
    "\n",
    "n = 4\n",
    "toy_indexes = torch.randint(0, dataset_size, (n,))\n",
    "toy_target = torch.rand((n,n))\n",
    "# symmetrize matrix\n",
    "toy_target += toy_target.T.clone()\n",
    "toy_target /= 2\n",
    "# set diagonal to ones\n",
    "eye_matrix = torch.eye(toy_target.size(0))\n",
    "toy_target += eye_matrix - torch.diag(torch.diag(toy_target))\n",
    "toy_indexes, toy_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.1135, 0.7251, 0.3034],\n",
      "        [0.5485, 1.0000, 0.4964, 0.0969],\n",
      "        [0.7759, 0.0304, 1.0000, 0.7615],\n",
      "        [0.3670, 0.8892, 0.2857, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(cos_sim_model(toy_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0100, 0.0100, 0.0100],\n",
      "        [0.0100, 1.0000, 0.0100, 0.0100],\n",
      "        [0.0100, 0.0100, 1.0000, 0.0100],\n",
      "        [0.0100, 0.0100, 0.0100, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 1.0000, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 1.0000, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0300, 0.0300, 0.0300],\n",
      "        [0.0300, 1.0000, 0.0300, 0.0300],\n",
      "        [0.0300, 0.0300, 1.0000, 0.0300],\n",
      "        [0.0300, 0.0300, 0.0300, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 1.0000, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 1.0000, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0500, 0.0500, 0.0500],\n",
      "        [0.0500, 1.0000, 0.0500, 0.0500],\n",
      "        [0.0500, 0.0451, 1.0000, 0.0500],\n",
      "        [0.0500, 0.0500, 0.0500, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0600, 0.0600, 0.0600],\n",
      "        [0.0600, 1.0000, 0.0600, 0.0600],\n",
      "        [0.0600, 0.0470, 1.0000, 0.0600],\n",
      "        [0.0600, 0.0600, 0.0600, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0700, 0.0700, 0.0700],\n",
      "        [0.0700, 1.0000, 0.0700, 0.0700],\n",
      "        [0.0700, 0.0466, 1.0000, 0.0700],\n",
      "        [0.0700, 0.0700, 0.0700, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0800, 0.0800, 0.0800],\n",
      "        [0.0800, 1.0000, 0.0800, 0.0800],\n",
      "        [0.0800, 0.0445, 1.0000, 0.0800],\n",
      "        [0.0800, 0.0800, 0.0800, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.0900, 0.0900, 0.0900],\n",
      "        [0.0900, 1.0000, 0.0900, 0.0900],\n",
      "        [0.0900, 0.0412, 1.0000, 0.0900],\n",
      "        [0.0900, 0.0900, 0.0900, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 1.0000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.0368, 1.0000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1100, 0.1100, 0.1100],\n",
      "        [0.1100, 1.0000, 0.1100, 0.1071],\n",
      "        [0.1100, 0.0316, 1.0000, 0.1100],\n",
      "        [0.1100, 0.1100, 0.1100, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1200, 0.1200, 0.1200],\n",
      "        [0.1200, 1.0000, 0.1200, 0.1118],\n",
      "        [0.1200, 0.0257, 1.0000, 0.1200],\n",
      "        [0.1200, 0.1200, 0.1200, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1273, 0.1300, 0.1300],\n",
      "        [0.1300, 1.0000, 0.1300, 0.1145],\n",
      "        [0.1300, 0.0220, 1.0000, 0.1300],\n",
      "        [0.1300, 0.1300, 0.1300, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1324, 0.1400, 0.1400],\n",
      "        [0.1400, 1.0000, 0.1400, 0.1156],\n",
      "        [0.1400, 0.0200, 1.0000, 0.1400],\n",
      "        [0.1400, 0.1400, 0.1400, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1356, 0.1500, 0.1500],\n",
      "        [0.1500, 1.0000, 0.1500, 0.1153],\n",
      "        [0.1500, 0.0195, 1.0000, 0.1500],\n",
      "        [0.1500, 0.1500, 0.1500, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1371, 0.1600, 0.1600],\n",
      "        [0.1600, 1.0000, 0.1600, 0.1138],\n",
      "        [0.1600, 0.0204, 1.0000, 0.1600],\n",
      "        [0.1600, 0.1600, 0.1600, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1373, 0.1700, 0.1700],\n",
      "        [0.1700, 1.0000, 0.1700, 0.1113],\n",
      "        [0.1700, 0.0223, 1.0000, 0.1700],\n",
      "        [0.1700, 0.1700, 0.1700, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1363, 0.1800, 0.1800],\n",
      "        [0.1800, 1.0000, 0.1800, 0.1079],\n",
      "        [0.1800, 0.0252, 1.0000, 0.1800],\n",
      "        [0.1800, 0.1800, 0.1800, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1342, 0.1900, 0.1900],\n",
      "        [0.1900, 1.0000, 0.1900, 0.1037],\n",
      "        [0.1900, 0.0289, 1.0000, 0.1900],\n",
      "        [0.1900, 0.1900, 0.1900, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1312, 0.2000, 0.2000],\n",
      "        [0.2000, 1.0000, 0.2000, 0.0989],\n",
      "        [0.2000, 0.0333, 1.0000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1275, 0.2100, 0.2100],\n",
      "        [0.2100, 1.0000, 0.2100, 0.0935],\n",
      "        [0.2100, 0.0361, 1.0000, 0.2100],\n",
      "        [0.2100, 0.2100, 0.2100, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1230, 0.2200, 0.2200],\n",
      "        [0.2200, 1.0000, 0.2200, 0.0898],\n",
      "        [0.2200, 0.0375, 1.0000, 0.2200],\n",
      "        [0.2200, 0.2200, 0.2200, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1180, 0.2300, 0.2300],\n",
      "        [0.2300, 1.0000, 0.2300, 0.0876],\n",
      "        [0.2300, 0.0376, 1.0000, 0.2300],\n",
      "        [0.2300, 0.2300, 0.2300, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1124, 0.2400, 0.2400],\n",
      "        [0.2400, 1.0000, 0.2400, 0.0867],\n",
      "        [0.2400, 0.0366, 1.0000, 0.2400],\n",
      "        [0.2400, 0.2400, 0.2400, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1085, 0.2500, 0.2500],\n",
      "        [0.2500, 1.0000, 0.2500, 0.0870],\n",
      "        [0.2500, 0.0347, 1.0000, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1060, 0.2600, 0.2600],\n",
      "        [0.2600, 1.0000, 0.2600, 0.0883],\n",
      "        [0.2600, 0.0319, 1.0000, 0.2600],\n",
      "        [0.2600, 0.2600, 0.2600, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1049, 0.2700, 0.2700],\n",
      "        [0.2700, 1.0000, 0.2700, 0.0906],\n",
      "        [0.2700, 0.0283, 1.0000, 0.2700],\n",
      "        [0.2700, 0.2700, 0.2700, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1050, 0.2800, 0.2800],\n",
      "        [0.2800, 1.0000, 0.2800, 0.0937],\n",
      "        [0.2800, 0.0262, 1.0000, 0.2800],\n",
      "        [0.2800, 0.2800, 0.2800, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1061, 0.2900, 0.2900],\n",
      "        [0.2900, 1.0000, 0.2900, 0.0975],\n",
      "        [0.2900, 0.0253, 1.0000, 0.2900],\n",
      "        [0.2900, 0.2900, 0.2900, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1081, 0.3000, 0.3000],\n",
      "        [0.3000, 1.0000, 0.3000, 0.0998],\n",
      "        [0.3000, 0.0256, 1.0000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.2979, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1110, 0.3100, 0.3100],\n",
      "        [0.3100, 1.0000, 0.3100, 0.1009],\n",
      "        [0.3100, 0.0269, 1.0000, 0.3100],\n",
      "        [0.3100, 0.3100, 0.3040, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1146, 0.3200, 0.3179],\n",
      "        [0.3200, 1.0000, 0.3200, 0.1008],\n",
      "        [0.3200, 0.0291, 1.0000, 0.3200],\n",
      "        [0.3200, 0.3200, 0.3083, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1168, 0.3300, 0.3240],\n",
      "        [0.3300, 1.0000, 0.3300, 0.0997],\n",
      "        [0.3300, 0.0320, 1.0000, 0.3300],\n",
      "        [0.3300, 0.3300, 0.3113, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1177, 0.3400, 0.3284],\n",
      "        [0.3400, 1.0000, 0.3400, 0.0977],\n",
      "        [0.3400, 0.0337, 1.0000, 0.3400],\n",
      "        [0.3400, 0.3400, 0.3128, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1176, 0.3500, 0.3314],\n",
      "        [0.3500, 1.0000, 0.3500, 0.0949],\n",
      "        [0.3500, 0.0342, 1.0000, 0.3500],\n",
      "        [0.3500, 0.3500, 0.3132, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1164, 0.3600, 0.3330],\n",
      "        [0.3600, 1.0000, 0.3600, 0.0934],\n",
      "        [0.3600, 0.0336, 1.0000, 0.3600],\n",
      "        [0.3600, 0.3600, 0.3125, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1143, 0.3700, 0.3334],\n",
      "        [0.3700, 1.0000, 0.3700, 0.0930],\n",
      "        [0.3700, 0.0320, 1.0000, 0.3700],\n",
      "        [0.3700, 0.3700, 0.3109, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1114, 0.3800, 0.3328],\n",
      "        [0.3800, 1.0000, 0.3800, 0.0937],\n",
      "        [0.3800, 0.0296, 1.0000, 0.3800],\n",
      "        [0.3780, 0.3800, 0.3084, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1098, 0.3900, 0.3312],\n",
      "        [0.3900, 1.0000, 0.3900, 0.0954],\n",
      "        [0.3900, 0.0284, 1.0000, 0.3900],\n",
      "        [0.3841, 0.3900, 0.3052, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1094, 0.4000, 0.3288],\n",
      "        [0.4000, 1.0000, 0.4000, 0.0979],\n",
      "        [0.4000, 0.0284, 1.0000, 0.4000],\n",
      "        [0.3886, 0.4000, 0.3013, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1101, 0.4100, 0.3256],\n",
      "        [0.4100, 1.0000, 0.4100, 0.0991],\n",
      "        [0.4100, 0.0293, 1.0000, 0.4100],\n",
      "        [0.3916, 0.4100, 0.2967, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1117, 0.4200, 0.3217],\n",
      "        [0.4200, 1.0000, 0.4200, 0.0992],\n",
      "        [0.4200, 0.0312, 1.0000, 0.4200],\n",
      "        [0.3933, 0.4200, 0.2916, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1141, 0.4300, 0.3172],\n",
      "        [0.4300, 1.0000, 0.4300, 0.0983],\n",
      "        [0.4300, 0.0319, 1.0000, 0.4300],\n",
      "        [0.3939, 0.4300, 0.2860, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1153, 0.4400, 0.3121],\n",
      "        [0.4400, 1.0000, 0.4400, 0.0964],\n",
      "        [0.4400, 0.0316, 1.0000, 0.4400],\n",
      "        [0.3933, 0.4400, 0.2820, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1154, 0.4500, 0.3066],\n",
      "        [0.4500, 1.0000, 0.4500, 0.0938],\n",
      "        [0.4500, 0.0302, 1.0000, 0.4500],\n",
      "        [0.3918, 0.4500, 0.2794, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1144, 0.4600, 0.3006],\n",
      "        [0.4600, 1.0000, 0.4600, 0.0924],\n",
      "        [0.4600, 0.0300, 1.0000, 0.4600],\n",
      "        [0.3895, 0.4600, 0.2781, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1126, 0.4700, 0.2962],\n",
      "        [0.4700, 1.0000, 0.4700, 0.0922],\n",
      "        [0.4700, 0.0308, 1.0000, 0.4700],\n",
      "        [0.3864, 0.4700, 0.2779, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1099, 0.4800, 0.2932],\n",
      "        [0.4800, 1.0000, 0.4800, 0.0929],\n",
      "        [0.4800, 0.0326, 1.0000, 0.4800],\n",
      "        [0.3826, 0.4800, 0.2787, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1085, 0.4900, 0.2916],\n",
      "        [0.4900, 1.0000, 0.4900, 0.0947],\n",
      "        [0.4900, 0.0331, 1.0000, 0.4900],\n",
      "        [0.3781, 0.4900, 0.2805, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1082, 0.5000, 0.2911],\n",
      "        [0.5000, 1.0000, 0.5000, 0.0972],\n",
      "        [0.5000, 0.0326, 1.0000, 0.5000],\n",
      "        [0.3731, 0.5000, 0.2831, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1090, 0.5100, 0.2917],\n",
      "        [0.5100, 1.0000, 0.5080, 0.0985],\n",
      "        [0.5100, 0.0312, 1.0000, 0.5100],\n",
      "        [0.3676, 0.5100, 0.2864, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1107, 0.5200, 0.2932],\n",
      "        [0.5200, 1.0000, 0.5142, 0.0987],\n",
      "        [0.5200, 0.0288, 1.0000, 0.5200],\n",
      "        [0.3637, 0.5200, 0.2904, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1132, 0.5300, 0.2956],\n",
      "        [0.5300, 1.0000, 0.5187, 0.0978],\n",
      "        [0.5300, 0.0278, 1.0000, 0.5300],\n",
      "        [0.3612, 0.5300, 0.2930, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1145, 0.5400, 0.2987],\n",
      "        [0.5400, 1.0000, 0.5218, 0.0960],\n",
      "        [0.5400, 0.0278, 1.0000, 0.5400],\n",
      "        [0.3599, 0.5400, 0.2943, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1146, 0.5500, 0.3025],\n",
      "        [0.5500, 1.0000, 0.5236, 0.0934],\n",
      "        [0.5500, 0.0288, 1.0000, 0.5500],\n",
      "        [0.3597, 0.5500, 0.2945, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1137, 0.5600, 0.3070],\n",
      "        [0.5580, 1.0000, 0.5242, 0.0921],\n",
      "        [0.5600, 0.0308, 1.0000, 0.5600],\n",
      "        [0.3606, 0.5600, 0.2937, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1119, 0.5700, 0.3100],\n",
      "        [0.5642, 1.0000, 0.5238, 0.0919],\n",
      "        [0.5700, 0.0335, 1.0000, 0.5700],\n",
      "        [0.3624, 0.5700, 0.2919, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1113, 0.5800, 0.3117],\n",
      "        [0.5688, 1.0000, 0.5223, 0.0927],\n",
      "        [0.5800, 0.0350, 1.0000, 0.5800],\n",
      "        [0.3650, 0.5800, 0.2893, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1118, 0.5900, 0.3122],\n",
      "        [0.5719, 1.0000, 0.5201, 0.0944],\n",
      "        [0.5900, 0.0353, 1.0000, 0.5900],\n",
      "        [0.3683, 0.5900, 0.2860, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1132, 0.6000, 0.3117],\n",
      "        [0.5737, 1.0000, 0.5170, 0.0970],\n",
      "        [0.6000, 0.0346, 1.0000, 0.6000],\n",
      "        [0.3723, 0.6000, 0.2840, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1135, 0.6100, 0.3102],\n",
      "        [0.5743, 1.0000, 0.5133, 0.0983],\n",
      "        [0.6100, 0.0329, 1.0000, 0.6100],\n",
      "        [0.3749, 0.6100, 0.2833, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1127, 0.6200, 0.3079],\n",
      "        [0.5738, 1.0000, 0.5089, 0.0985],\n",
      "        [0.6200, 0.0304, 1.0000, 0.6200],\n",
      "        [0.3763, 0.6200, 0.2836, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1110, 0.6300, 0.3048],\n",
      "        [0.5724, 1.0000, 0.5040, 0.0976],\n",
      "        [0.6300, 0.0292, 1.0000, 0.6300],\n",
      "        [0.3765, 0.6300, 0.2848, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1105, 0.6400, 0.3030],\n",
      "        [0.5701, 1.0000, 0.4985, 0.0959],\n",
      "        [0.6400, 0.0291, 1.0000, 0.6400],\n",
      "        [0.3757, 0.6400, 0.2870, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1110, 0.6500, 0.3024],\n",
      "        [0.5671, 1.0000, 0.4926, 0.0933],\n",
      "        [0.6500, 0.0300, 1.0000, 0.6500],\n",
      "        [0.3740, 0.6500, 0.2879, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1125, 0.6600, 0.3028],\n",
      "        [0.5634, 1.0000, 0.4883, 0.0920],\n",
      "        [0.6600, 0.0318, 1.0000, 0.6600],\n",
      "        [0.3714, 0.6600, 0.2877, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1129, 0.6700, 0.3042],\n",
      "        [0.5590, 1.0000, 0.4854, 0.0918],\n",
      "        [0.6700, 0.0324, 1.0000, 0.6700],\n",
      "        [0.3681, 0.6700, 0.2866, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1122, 0.6800, 0.3065],\n",
      "        [0.5541, 1.0000, 0.4839, 0.0926],\n",
      "        [0.6800, 0.0320, 1.0000, 0.6800],\n",
      "        [0.3661, 0.6800, 0.2865, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1125, 0.6900, 0.3075],\n",
      "        [0.5486, 1.0000, 0.4834, 0.0943],\n",
      "        [0.6900, 0.0306, 1.0000, 0.6900],\n",
      "        [0.3653, 0.6900, 0.2875, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1119, 0.7000, 0.3075],\n",
      "        [0.5428, 1.0000, 0.4840, 0.0969],\n",
      "        [0.7000, 0.0304, 1.0000, 0.7000],\n",
      "        [0.3657, 0.7000, 0.2874, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1123, 0.7100, 0.3064],\n",
      "        [0.5385, 1.0000, 0.4856, 0.0982],\n",
      "        [0.7100, 0.0312, 1.0000, 0.7100],\n",
      "        [0.3669, 0.7100, 0.2863, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1136, 0.7200, 0.3045],\n",
      "        [0.5356, 1.0000, 0.4880, 0.0984],\n",
      "        [0.7200, 0.0308, 1.0000, 0.7200],\n",
      "        [0.3691, 0.7200, 0.2863, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1139, 0.7280, 0.3037],\n",
      "        [0.5340, 1.0000, 0.4912, 0.0976],\n",
      "        [0.7300, 0.0316, 1.0000, 0.7300],\n",
      "        [0.3700, 0.7300, 0.2873, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1131, 0.7342, 0.3040],\n",
      "        [0.5336, 1.0000, 0.4950, 0.0958],\n",
      "        [0.7400, 0.0312, 1.0000, 0.7400],\n",
      "        [0.3698, 0.7400, 0.2871, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1113, 0.7388, 0.3053],\n",
      "        [0.5342, 1.0000, 0.4975, 0.0932],\n",
      "        [0.7500, 0.0299, 1.0000, 0.7500],\n",
      "        [0.3687, 0.7500, 0.2861, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1108, 0.7419, 0.3055],\n",
      "        [0.5357, 1.0000, 0.4987, 0.0919],\n",
      "        [0.7600, 0.0297, 1.0000, 0.7600],\n",
      "        [0.3667, 0.7600, 0.2861, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1113, 0.7437, 0.3046],\n",
      "        [0.5381, 1.0000, 0.4988, 0.0917],\n",
      "        [0.7700, 0.0306, 1.0000, 0.7700],\n",
      "        [0.3658, 0.7700, 0.2871, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1128, 0.7443, 0.3048],\n",
      "        [0.5413, 1.0000, 0.4978, 0.0926],\n",
      "        [0.7800, 0.0323, 1.0000, 0.7780],\n",
      "        [0.3661, 0.7800, 0.2870, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1131, 0.7439, 0.3061],\n",
      "        [0.5452, 1.0000, 0.4960, 0.0943],\n",
      "        [0.7900, 0.0329, 1.0000, 0.7842],\n",
      "        [0.3673, 0.7900, 0.2859, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1124, 0.7425, 0.3061],\n",
      "        [0.5496, 1.0000, 0.4934, 0.0969],\n",
      "        [0.7980, 0.0324, 1.0000, 0.7888],\n",
      "        [0.3694, 0.8000, 0.2860, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1127, 0.7402, 0.3052],\n",
      "        [0.5526, 1.0000, 0.4900, 0.0982],\n",
      "        [0.8042, 0.0310, 1.0000, 0.7919],\n",
      "        [0.3703, 0.8100, 0.2870, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1120, 0.7372, 0.3034],\n",
      "        [0.5543, 1.0000, 0.4880, 0.0984],\n",
      "        [0.8088, 0.0287, 1.0000, 0.7937],\n",
      "        [0.3701, 0.8200, 0.2869, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1124, 0.7335, 0.3027],\n",
      "        [0.5549, 1.0000, 0.4871, 0.0975],\n",
      "        [0.8119, 0.0277, 1.0000, 0.7943],\n",
      "        [0.3690, 0.8300, 0.2858, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1118, 0.7291, 0.3032],\n",
      "        [0.5544, 1.0000, 0.4874, 0.0958],\n",
      "        [0.8137, 0.0277, 1.0000, 0.7939],\n",
      "        [0.3669, 0.8400, 0.2859, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1122, 0.7242, 0.3045],\n",
      "        [0.5529, 1.0000, 0.4886, 0.0932],\n",
      "        [0.8143, 0.0287, 1.0000, 0.7925],\n",
      "        [0.3660, 0.8500, 0.2869, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1135, 0.7188, 0.3068],\n",
      "        [0.5506, 1.0000, 0.4907, 0.0919],\n",
      "        [0.8139, 0.0307, 1.0000, 0.7903],\n",
      "        [0.3663, 0.8600, 0.2868, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1138, 0.7129, 0.3078],\n",
      "        [0.5475, 1.0000, 0.4936, 0.0917],\n",
      "        [0.8125, 0.0334, 1.0000, 0.7872],\n",
      "        [0.3675, 0.8700, 0.2858, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1130, 0.7086, 0.3077],\n",
      "        [0.5437, 1.0000, 0.4952, 0.0925],\n",
      "        [0.8103, 0.0349, 1.0000, 0.7835],\n",
      "        [0.3696, 0.8800, 0.2858, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1113, 0.7057, 0.3066],\n",
      "        [0.5413, 1.0000, 0.4956, 0.0943],\n",
      "        [0.8072, 0.0352, 1.0000, 0.7791],\n",
      "        [0.3705, 0.8880, 0.2869, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1107, 0.7042, 0.3046],\n",
      "        [0.5402, 1.0000, 0.4950, 0.0969],\n",
      "        [0.8035, 0.0345, 1.0000, 0.7742],\n",
      "        [0.3703, 0.8942, 0.2868, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1113, 0.7037, 0.3039],\n",
      "        [0.5401, 1.0000, 0.4935, 0.0982],\n",
      "        [0.7991, 0.0329, 1.0000, 0.7688],\n",
      "        [0.3691, 0.8988, 0.2857, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1127, 0.7044, 0.3042],\n",
      "        [0.5411, 1.0000, 0.4911, 0.0984],\n",
      "        [0.7942, 0.0304, 1.0000, 0.7629],\n",
      "        [0.3670, 0.9019, 0.2858, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1130, 0.7059, 0.3054],\n",
      "        [0.5430, 1.0000, 0.4899, 0.0975],\n",
      "        [0.7888, 0.0292, 1.0000, 0.7586],\n",
      "        [0.3661, 0.9037, 0.2868, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1123, 0.7083, 0.3056],\n",
      "        [0.5456, 1.0000, 0.4899, 0.0958],\n",
      "        [0.7829, 0.0291, 1.0000, 0.7558],\n",
      "        [0.3664, 0.9043, 0.2868, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1127, 0.7115, 0.3047],\n",
      "        [0.5491, 1.0000, 0.4909, 0.0932],\n",
      "        [0.7786, 0.0300, 1.0000, 0.7542],\n",
      "        [0.3676, 0.9039, 0.2857, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1120, 0.7153, 0.3049],\n",
      "        [0.5511, 1.0000, 0.4927, 0.0919],\n",
      "        [0.7758, 0.0318, 1.0000, 0.7538],\n",
      "        [0.3696, 0.9025, 0.2858, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1124, 0.7198, 0.3061],\n",
      "        [0.5520, 1.0000, 0.4954, 0.0917],\n",
      "        [0.7742, 0.0324, 1.0000, 0.7544],\n",
      "        [0.3705, 0.9003, 0.2868, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1117, 0.7228, 0.3062],\n",
      "        [0.5518, 1.0000, 0.4968, 0.0925],\n",
      "        [0.7738, 0.0320, 1.0000, 0.7559],\n",
      "        [0.3703, 0.8972, 0.2867, 1.0000]], grad_fn=<IndexBackward0>)\n",
      "tensor([[1.0000, 0.1121, 0.7245, 0.3053],\n",
      "        [0.5506, 1.0000, 0.4971, 0.0943],\n",
      "        [0.7744, 0.0306, 1.0000, 0.7583],\n",
      "        [0.3691, 0.8935, 0.2857, 1.0000]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    sub_matrix = cos_sim_model(toy_indexes)\n",
    "    print(sub_matrix)\n",
    "    loss = (sub_matrix - toy_target).abs().mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_simclr import SimCLR\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "mod = SimCLR.load_from_checkpoint(\"logs/No_schedule-_No_decay-1000/l2/SimCLR-1000/checkpoint/epoch=999.ckpt\", strict=False)\n",
    "\n",
    "cifar10_train = CIFAR10(\n",
    "    root=\".\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "X, y, Z = mod.embed_dataset(cifar10_train)\n",
    "embedded_dataset = Z\n",
    "labels = y\n",
    "\n",
    "variances = []\n",
    "for i in list(set(labels)):\n",
    "    variances.append(embedded_dataset[labels == i].var(axis=0))\n",
    "\n",
    "variances = torch.tensor(np.stack(variances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0938)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECTOR_HIDDEN_SIZE = 1024\n",
    "\n",
    "class ResNet18withProjector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = resnet18(weights=None)\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.backbone.maxpool = nn.Identity()\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, PROJECTOR_HIDDEN_SIZE), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(PROJECTOR_HIDDEN_SIZE, 128),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        z = self.projector(h)\n",
    "        self.embed = z # save for monitoring\n",
    "        return h, z\n",
    "    \n",
    "def dataset_to_X_y(model, dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    Z = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(DataLoader(dataset, batch_size=1024)):\n",
    "        images, labels = batch\n",
    "\n",
    "        h, z = model(images.to(device))\n",
    "\n",
    "        X.append(h.cpu().numpy())\n",
    "        Z.append(z.cpu().numpy())\n",
    "        y.append(labels)\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    Z = np.vstack(Z)\n",
    "    y = np.hstack(y)\n",
    "\n",
    "    return X, y, Z\n",
    "\n",
    "untrained_model = ResNet18withProjector().to(device)\n",
    "untrained_model.eval()\n",
    "with torch.no_grad():\n",
    "    X, y, Z = dataset_to_X_y(untrained_model, cifar10_train)\n",
    "untrained_model.train()\n",
    "embedded_dataset_untrained = Z\n",
    "embedded_dataset_untrained = torch.tensor(embedded_dataset_untrained)\n",
    "embedded_dataset_untrained.norm(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class AugmentedPEM(nn.Module): # Augmented Plain Embedding Model\n",
    "    def __init__(\n",
    "            self,\n",
    "            seed = 0,\n",
    "            noise_dist_per_label = variances,\n",
    "            noise_strength = 1,\n",
    "            embeddings = None\n",
    "            ):\n",
    "        torch.manual_seed(seed)\n",
    "        super().__init__()\n",
    "        self.embeds = nn.Embedding(50000, 128)\n",
    "        if embeddings is not None:\n",
    "            with torch.no_grad(): # make embeddings very similar to each other\n",
    "                self.embeds.weight = nn.Parameter(copy.deepcopy(embeddings))\n",
    "        self.labels = torch.tensor(cifar10_train.targets)\n",
    "        self.noise_dist_per_label = noise_dist_per_label\n",
    "        self.noise_strength = noise_strength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeds.weight)\n",
    "\n",
    "    def __getitem__(self, *i):\n",
    "        embed = self.embeds.weight[i]\n",
    "        label = self.labels[i]\n",
    "        noise_dist_per_embed = self.noise_dist_per_label[label]\n",
    "        view1 = embed + torch.randn_like(noise_dist_per_embed) * noise_dist_per_embed * self.noise_strength\n",
    "        view2 = embed + torch.randn_like(noise_dist_per_embed) * noise_dist_per_embed * self.noise_strength\n",
    "\n",
    "        return view1, view2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean embed norm: 2.263796329498291\n",
      "Epoch 1, average loss 6.9113, 0.7 s\n",
      "Mean embed norm: 3.8354976177215576\n",
      "Epoch 11, average loss 4.9620, 0.5 s\n",
      "Mean embed norm: 3.8632261753082275\n",
      "Epoch 21, average loss 4.9500, 0.5 s\n",
      "Mean embed norm: 3.8865113258361816\n",
      "Epoch 31, average loss 4.9464, 0.4 s\n",
      "Mean embed norm: 3.9084367752075195\n",
      "Epoch 41, average loss 4.9446, 0.4 s\n",
      "Mean embed norm: 3.929546356201172\n",
      "Epoch 51, average loss 4.9436, 0.5 s\n",
      "Mean embed norm: 3.949846029281616\n",
      "Epoch 61, average loss 4.9430, 0.4 s\n",
      "Mean embed norm: 3.969762086868286\n",
      "Epoch 71, average loss 4.9427, 0.4 s\n",
      "Mean embed norm: 3.9893405437469482\n",
      "Epoch 81, average loss 4.9426, 0.5 s\n",
      "Mean embed norm: 4.008326053619385\n",
      "Epoch 91, average loss 4.9424, 0.5 s\n",
      "Total training length for 100 epochs: 0h 1min\n"
     ]
    }
   ],
   "source": [
    "from losses import nt_xent\n",
    "import time\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 100\n",
    "N_EPOCHS_COS_ANNEAL = 1000\n",
    "BASE_LR = 1000\n",
    "WEIGHT_DECAY = 0#5e-4\n",
    "MOMENTUM = 0#.9\n",
    "NESTEROV = False\n",
    "PRINT_EVERY_EPOCHS = 10\n",
    "\n",
    "NOISE_STRENGTH = 0\n",
    "\n",
    "\n",
    "ag_pem = AugmentedPEM(noise_strength=NOISE_STRENGTH, embeddings=embedded_dataset_untrained)\n",
    "\n",
    "\n",
    "dataset_size = len(ag_pem)\n",
    "\n",
    "optimizer = SGD(\n",
    "    ag_pem.parameters(),\n",
    "    lr=BASE_LR * BATCH_SIZE / 256,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    nesterov=NESTEROV,\n",
    ")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=N_EPOCHS_COS_ANNEAL)\n",
    "\n",
    "\n",
    "norm_history = []\n",
    "loss_history = []\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch_idx in range(N_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    indices = torch.randperm(dataset_size)\n",
    "    batch_ids = [indices[i:i+BATCH_SIZE] for i in range(0, dataset_size, BATCH_SIZE)]\n",
    "    # batches = [ag_pem[batch_id] for batch_id in batch_ids]\n",
    "\n",
    "    for batch_idx, batch_id in enumerate(batch_ids):\n",
    "        batch = ag_pem[batch_id]\n",
    "        view1, view2 , labels = batch\n",
    "        view1 = view1.to(device)\n",
    "        view2 = view2.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        infoNCE = nt_xent\n",
    "        loss = infoNCE(view1, view2)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    loss_history.append(epoch_loss/ len(batch_ids))\n",
    "    mean_norm = ag_pem.embeds.weight.norm(dim=-1).mean().item()\n",
    "    norm_history.append(ag_pem.embeds.weight.norm(dim=-1))\n",
    "\n",
    "    if (epoch_idx) % PRINT_EVERY_EPOCHS == 0:\n",
    "        print(\"Mean embed norm:\", mean_norm)\n",
    "        print(\n",
    "            f\"Epoch {epoch_idx + 1}, \"\n",
    "            f\"average loss {epoch_loss / len(batch_ids):.4f}, \"\n",
    "            f\"{end_time - start_time:.1f} s\",\n",
    "            flush=True\n",
    "        )\n",
    "    scheduler.step()\n",
    "\n",
    "norm_history = torch.stack(norm_history)\n",
    "\n",
    "training_end_time = time.time()\n",
    "hours = (training_end_time - training_start_time) / 60 // 60\n",
    "minutes = (training_end_time - training_start_time) / 60 % 60\n",
    "print(\n",
    "    f\"Total training length for {N_EPOCHS} epochs: {hours:.0f}h {minutes:.0f}min\",\n",
    "    flush=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], grad_fn=<SortBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_diff = norm_history[1:] - norm_history[0:-1]\n",
    "epoch_diff[epoch_diff < 0].abs().sort().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.9075701236724854, 3.9075698852539062)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = 0\n",
    "norm_history[pos,823].item(), norm_history[pos+1,823].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  0,  ..., 98, 98, 98]),\n",
       " tensor([  823,  3781,  3848,  ..., 47243, 48324, 49183]),\n",
       " 5395)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords = torch.nonzero(~(norm_history[1:] >= norm_history[0:-1]), as_tuple=False)\n",
    "\n",
    "x_vals = coords[:, 0]\n",
    "y_vals = coords[:, 1]\n",
    "\n",
    "x_vals, y_vals, len(x_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGiCAYAAAA1LsZRAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKWxJREFUeJzt3Xtw1fWd//HXSU7OSUhyTkKQRCAgLRREpNxUgv3V/hasF7aF3R2nZdiCrrqji7OwzmwtVre/egs7bLe6a4vaXWRnlGVqf4JdpizDYpH6AzEgtohb1FUJWpJwS04SknNOzvn8/si5hiTk5PL9kJznY+ZMku/5JufDZxzfr/ncvi5jjBEAAIAlObYbAAAAshthBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFg1oDCyfv16uVwurV27tsd7Nm/eLJfLlfbKz88fyMcCAIARxN3fX6ypqdHzzz+vWbNmXfJen8+n48ePJ352uVz9/VgAADDC9GtkpKWlRStWrNDPfvYzlZaWXvJ+l8ulioqKxKu8vLw/HwsAAEagfo2MrF69WkuWLNHixYv1xBNPXPL+lpYWTZo0SdFoVHPnztVTTz2la665psf7g8GggsFg4udoNKpz586prKyMURUAAIYJY4yam5s1btw45eT0PP6RcRjZunWr3nnnHdXU1PTp/mnTpmnTpk2aNWuWmpqa9A//8A9auHChjh07pgkTJnT7O9XV1frhD3+YadMAAMBl6OTJkz3WfElyGWNMJn9s/vz52r17d2KtyNe+9jXNnj1bTz/9dJ/+Rjgc1tVXX63ly5fr8ccf7/aeriMjTU1Nmjhxok6ePCmfz9fX5gIAAIsCgYAqKyvV2Ngov9/f430ZjYwcPnxYDQ0Nmjt3buJaJBLRvn379OyzzyoYDCo3N7fXv5GXl6c5c+boo48+6vEer9crr9d70XWfz0cYAQBgmLnUEouMwsiiRYt09OjRtGt33XWXpk+froceeuiSQUTqDC9Hjx7V7bffnslHAwCAESqjMFJcXKyZM2emXSssLFRZWVni+sqVKzV+/HhVV1dLkh577DEtWLBAU6ZMUWNjozZs2KATJ07onnvuGaR/AgAAGM76fc5IT2pra9NWzJ4/f1733nuv6urqVFpaqnnz5mn//v2aMWPGYH80AAAYhjJawGpLIBCQ3+9XU1MTa0YAABgm+lq/eTYNAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqt+0GAAAAZ0WjRi2hDgXawgq0dai5Paz5V41Wbo7LSnsIIwAADDOpYaIpFigC7eHOcNEeCxntyaARaA+rqS15vSXYIWPS/+aRR29WaaHHyr9nQGFk/fr1WrdundasWaOnn366x/teeeUVPfroo/r00081depU/f3f/71uv/32gXw0AADDVteRiZ6CRPJ6+s/N3YSJ/vDk5shXkCdfvlvtHZGB/8F+6ncYqamp0fPPP69Zs2b1et/+/fu1fPlyVVdX64//+I+1ZcsWLVu2TO+8845mzpzZ348HAMAaY4zaw9HOUYn2+OhE8ms8VDR1CRLx9wcrTHjdnWHCHwsUxfmd3xfnu+WLf41di4eOzq+d7+Xn5Q68EYPAZUzm3dHS0qK5c+fqpz/9qZ544gnNnj27x5GRb33rW2ptbdWOHTsS1xYsWKDZs2frueee69PnBQIB+f1+NTU1yefzZdpcAAAuEuqIXhwk2jtSAkW415GLcGTgaSIxMlHQGRp8XUJF6vXifHcsdCQDx+USJnrS1/rdr5GR1atXa8mSJVq8eLGeeOKJXu89cOCAHnzwwbRrt9xyi7Zv397j7wSDQQWDwcTPgUCgP80EAIxgxhhdCEXUFAsSqa9ADwEjdSSjPRwdcBtyc1zyxUNCSlDwFSRHIFJHLrqOTlzuYcIpGYeRrVu36p133lFNTU2f7q+rq1N5eXnatfLyctXV1fX4O9XV1frhD3+YadMAAMNMNGrUHExOaTS1hdV4oUuwaO8+YATawuqIDnx0InUqI3U0ouvURnE394zy5MrlsrMDZSTJKIycPHlSa9as0e7du5Wfnz9UbdK6devSRlMCgYAqKyuH7PMAAP0XiRo1t3cdmejodrSi67Xm9rAGmifycl2J4JAIC/kXB4vu7inKd1vbzoqkjMLI4cOH1dDQoLlz5yauRSIR7du3T88++6yCwaByc9OHnCoqKlRfX592rb6+XhUVFT1+jtfrldfrzaRpAIABCEeiaWGhu6mN7sJE04XBWYzpdeeoZFRnSCgp8KSHhoL0tRL+UelhoyCP0YnhLqMwsmjRIh09ejTt2l133aXp06froYceuiiISFJVVZX27NmjtWvXJq7t3r1bVVVV/WsxAKBb4Ui059GICxdPezReSL7fGhr4ts6CvNxEgEhMccSCREmBR/4Ct/yj8i6+h7UTWS+jMFJcXHzRdtzCwkKVlZUlrq9cuVLjx49XdXW1JGnNmjW66aab9KMf/UhLlizR1q1bdejQIb3wwguD9E8AgJGjIxJVc3v6FEdjN8GisS0Ue79DTRdCgxYoirzulOkMd5cpj4uDROrPHjdPGEH/DPoJrLW1tcrJSf4HuXDhQm3ZskWPPPKIHn74YU2dOlXbt2/njBEAI5YxRi3BjsRCzMaU8JA6GtF1oWZTW+fJmAMV3wLa3cvXTYhIrqFwy51LoIDz+nXOiNM4ZwSA0+LbRhu7TnF0s27iopGLtrAiA1yVWejJ7Xb0Ib6uwj/Kk7zWJWywIBOXiyE9ZwQAhoto1Ki5vUONbSE1Xgjr/IXkCEVi5KItpEBiBCN+PTTgQ63y83JiayWSCy+7DRbdjF7kMUKBLEIYAXDZM8aoLRxJm/boHIEIpf3cdRQjvlhzIOO/XbeNlnQTHJLhwpMWNFiUCfQNYQSAY+LnUSRHINJHKRrbQrHFmfHQEUqEioGOUozy5KokFhhKRsVHJTxpIxSlsWvJ9/PYNgo4gDACIGMdsS2kqVMa51vjIxOhxPWuPw90lMKd40pMd5R0GYmIh4quUx/xkQuvm1EK4HJFGAGyWCRqFGjrXEcRn+Jo7BIszqesszh/oXPdRXP7wHZ8xBdnpo9SxH4uSB+1SF1rUcjR28CIRBgBRoD48z0aY+GhMRYa4j/HpzzOp0yPDMZIRXwLaWkiVHgSayrSfh6VnALhPAoAXRFGgMtMeziSGIGIfz3XGkqEifMpQSN17cVAdpIWed2JEYrSUcl1FCUFyZBROio5klE6ysOZFAAGDWEEGCKRqEkfkUj7mvJ9azJQnL8QGtBjzVMXaZYWxo7gjq2vSB29KB2VHLnws40UgGWEEaAPgh2RxAjF+ViA6ByhCOlcazcBY4BTILk5rti0Rp5GF3oSAaK00KOSgvRRitTRDLaSAhiOCCPIOqGOaGeIiI9KpEx/nG/tvH6utfP7s62dUyEDOaK72OuOrZlITnGUpoxQlMbCRmL0ojBPxV43CzUBZA3CCIa1SNQkRiXOtsS+xoJEfMTiXErIaGztfNx5f+S4lAgU8dGK0bHwkBowRhd60s6zYAoEAHpHGMFlw5jOHSGdQaIzWJxrDetcazDta+oIRlNb/6ZCclxKW4xZmjJKMbqwM2R0fp+n0YVejR7lUXG+Wzk88wMABh1hBEMmGjVqbAvrbEtQZ2MBIzlqEYpdC+psSzJ89PeUTV++W2VFXo0u7AwWowtjwSIeKmILOktjIxe+/DyCBQBcJggj6LP4Y9HPtoR0tjWoMy0hnWnpDBPxwBF/Lz5l0p/tpqM8uYnQUFroUVk3AWN0YfL9koI8tpgCwDBGGIFagx060xLU6eZg4uvp5qBOtwR1ujmk0y1BnYm9F+zIfNupvyBPZUXJEFFW5EmEjfirrNCr0UWdwYMdIQCQXQgjI1Q0anS2NaT6QLtONwdVH2hXQ3PXoNH5agtHMvrbBXm5GlPcGSDGFHk1pqgzYJQVelVW5NGY2HRJPHSwgBMA0BvCyDAU6oiqPtCuU03tOtXUplNN7aqLvwLtieARyWCOJB4wxhR5NbbYqyuKvbqiKF9XFHeGjTHFXo0p9GpMsUejPPxnAwAYPFSVy1CoI6rPG9tUe+6CTp67oJPnL+jz8236vLFNn59v0+mWYJ92kLhcUllhZ7go93k1tjhfY33xoOGNBY3Or4Ve/lMAANhBBbIkEjX67PwF/c/pFn1y5oI+OdOiT8606tMzF3Sqqe2SCz897hxd6c+PvQpU4c9XhS9f5b58lfu8qvDn64oiLws7AQCXPcKIA5rbw/rdZ0363WdN+qC+WR/UN+t/Trf0+gySgrxcTRw9SpWjR2lCaYEmlBZofEmBxpcWaFxJgcoKPZzQCQAYEQgjQ+APjW36zYendejT83r3ZKM+Ot3S7bSKx52jL4wp1BeuKNTkMYW6qqzz66SyQo0pImwAALIDYWQQhDqiOvDxWe374LT2fXBaHza0XHTPhNICfbmyRFdXFGtqebG+VF6siaNHKZeDtwAAWY4wMgANgXa9fLBWW96u1enmYOJ6jkuaXVmiqi+WaU5lqb5cWaIrir0WWwoAwOWLMNIPR2rPa9P/+1Q7j55SR2yl6ZgirxZNH6uvfukKfWXKGPlH5VluJQAAwwNhJAPGGP107/9ow67jiWvzJ5Vq1cKrdMs1FfK42bkCAECmCCN9FOqIat2rR/V/3/lMkrR09jjd+7++oJnj/ZZbBgDA8EYY6YPzrSHd99JhHfzknHJzXPo/37xG31kwyXazAAAYEQgjl/DpmVbdtblGn5xpVbHXrWdXzNVNX7rCdrMAABgxCCOX8INfHtMnZ1o1vqRAm+68TtMqim03CQCAEYUVl5fwYX2zJOnpb88miAAAMAQII70wxuh0S+f5IeNKCiy3BgCAkYkw0ovzF8IKRzrPEbmiiEPLAAAYCoSRXjQ0t0uSSkflcYYIAABDhArbi4ZA5xTN2OJ8yy0BAGDkIoz0oiH2vJmxPqZoAAAYKoSRXsSnaXjIHQAAQ4cw0gumaQAAGHqEkV6cjk3TlDNNAwDAkCGM9CI+TcPICAAAQ4cw0gsWsAIAMPQIIz0wxqSsGSGMAAAwVAgjPWgJdqgtHJHENA0AAEOJMNKD+BRNsdetAk+u5dYAADByEUZ6UB+InTHCehEAAIYUYaQH8W29rBcBAGBoEUZ6wIFnAAA4gzDSg+QZI4yMAAAwlAgjPeCMEQAAnEEY6QHTNAAAOIMw0gOmaQAAcAZhpAdM0wAA4AzCSDfawxE1t3dIkq5gmgYAgCFFGOlGfL2I150jX77bcmsAABjZCCPdSKwX8XnlcrkstwYAgJGNMNKNxHoRpmgAABhyhJFuNMSeS1PO4lUAAIYcYaQbjIwAAOAcwkg36mMLWK/gjBEAAIYcYaQbHHgGAIBzCCPdOJ048IxpGgAAhhphpBvJNSOMjAAAMNQII12EOqI61xqSRBgBAMAJhJEuzrR0joq4c1wqHeWx3BoAAEY+wkgX8SmaK4q9ysnh9FUAAIYaYaSL+IFnTNEAAOAMwkgXyZERdtIAAOCEjMLIxo0bNWvWLPl8Pvl8PlVVVWnnzp093r9582a5XK60V37+5V3kEztpOAoeAABHuDO5ecKECVq/fr2mTp0qY4z+7d/+TUuXLtWRI0d0zTXXdPs7Pp9Px48fT/x8uT8F9zQHngEA4KiMwsg3vvGNtJ+ffPJJbdy4UW+99VaPYcTlcqmioqL/LXRYQ4Dn0gAA4KR+rxmJRCLaunWrWltbVVVV1eN9LS0tmjRpkiorK7V06VIdO3bskn87GAwqEAikvZzCgWcAADgr4zBy9OhRFRUVyev16r777tO2bds0Y8aMbu+dNm2aNm3apNdee00vvfSSotGoFi5cqM8++6zXz6iurpbf70+8KisrM21mvyWeS8OaEQAAHOEyxphMfiEUCqm2tlZNTU36xS9+oX/5l3/RG2+80WMgSRUOh3X11Vdr+fLlevzxx3u8LxgMKhgMJn4OBAKqrKxUU1OTfD5fJs3NSCRq9KVHdioSNXpr3SJV+JmqAQCgvwKBgPx+/yXrd0ZrRiTJ4/FoypQpkqR58+appqZGzzzzjJ5//vlL/m5eXp7mzJmjjz76qNf7vF6vvF7nRybOtgYViRq5XNKYIk5fBQDACQM+ZyQajaaNYvQmEono6NGjuvLKKwf6sUMivni1rNArdy5HsAAA4ISMRkbWrVun2267TRMnTlRzc7O2bNmivXv3ateuXZKklStXavz48aqurpYkPfbYY1qwYIGmTJmixsZGbdiwQSdOnNA999wz+P+SQXCaxasAADguozDS0NCglStX6tSpU/L7/Zo1a5Z27dqlm2++WZJUW1urnJzkiML58+d17733qq6uTqWlpZo3b57279/fp/UlNjS1hSVJpYV5llsCAED2yHgBqw19XQAzUD8/dFLf/cXv9L+nXaEX77p+yD4HAIBs0Nf6zcKIFKGOqCTJ46ZbAABwClU3RTjSGUbyWLwKAIBjqLop4mHEQxgBAMAxVN0UTNMAAOA8qm6KUKRzLS/TNAAAOIeqm4I1IwAAOI+qmyI+TZPndlluCQAA2YMwkiI+MuJlZAQAAMdQdVMwTQMAgPOouimCiWkaugUAAKdQdVOEY7tpOGcEAADnUHVThBkZAQDAcVTdFMkTWNlNAwCAUwgjKUIRTmAFAMBpVN0UiXNGWDMCAIBjqLop2NoLAIDzqLopmKYBAMB5VN0U4Q629gIA4DSqbgqmaQAAcB5VN0XiBFa29gIA4BjCSIowa0YAAHAcVTdF8tAzugUAAKdQdVNwzggAAM6j6qZIPCiPaRoAABxD1Y0xxiTOGWFkBAAA51B1YzqiJvE9a0YAAHAOVTcmvl5EYpoGAAAnUXVj4jtpJM4ZAQDASYSRmPh6EZdLys0hjAAA4BTCSEzqtl6XizACAIBTCCMx8W29XhavAgDgKCpvTOIheSxeBQDAUVTemBAPyQMAwArCSEyIh+QBAGAFlTcmzHNpAACwgsobE+KJvQAAWEHljQkzTQMAgBVU3phQR+fWXqZpAABwFpU3JrG1l900AAA4ijASE9/a63HnWm4JAADZhTASk1gzwsgIAACOIozEJKdp6BIAAJxE5Y0Jcs4IAABWUHlj4g/KY2svAADOovLGME0DAIAdVN6YxG4aFrACAOAowkgMJ7ACAGAHlTcmxDQNAABWUHljQuymAQDACipvDNM0AADYQeWNSWztZWQEAABHUXljQjwoDwAAKwgjMYk1I0zTAADgKCpvTPJBeXQJAABOovLGsIAVAAA7qLwxbO0FAMAOKm9MiN00AABYQeWNCbOAFQAAK6i8MWztBQDADsJITHwBq5eREQAAHEXljQmzgBUAACuovDE8tRcAADuovDHxrb2cMwIAgLOovDE8KA8AADuovDFhpmkAALCCyispGjXqiHaOjLC1FwAAZ2UURjZu3KhZs2bJ5/PJ5/OpqqpKO3fu7PV3XnnlFU2fPl35+fm69tpr9atf/WpADR4K8cWrEmtGAABwWkaVd8KECVq/fr0OHz6sQ4cO6Y/+6I+0dOlSHTt2rNv79+/fr+XLl+vuu+/WkSNHtGzZMi1btkzvvffeoDR+sIRTwgjTNAAAOMtljDED+QOjR4/Whg0bdPfdd1/03re+9S21trZqx44diWsLFizQ7Nmz9dxzz/X5MwKBgPx+v5qamuTz+QbS3G6dbQlq3hP/JUn6+KnblZPDVA0AAAPV1/rd72GASCSirVu3qrW1VVVVVd3ec+DAAS1evDjt2i233KIDBw70+reDwaACgUDaayjFd9K4c1wEEQAAHJZxGDl69KiKiork9Xp13333adu2bZoxY0a399bV1am8vDztWnl5uerq6nr9jOrqavn9/sSrsrIy02ZmhJ00AADYk3H1nTZtmt59910dPHhQ999/v1atWqX3339/UBu1bt06NTU1JV4nT54c1L/fVbCDh+QBAGCLO9Nf8Hg8mjJliiRp3rx5qqmp0TPPPKPnn3/+onsrKipUX1+fdq2+vl4VFRW9fobX65XX6820af0WHxnxuHMd+0wAANBpwPMS0WhUwWCw2/eqqqq0Z8+etGu7d+/ucY2JLYkwwsgIAACOy2hkZN26dbrttts0ceJENTc3a8uWLdq7d6927dolSVq5cqXGjx+v6upqSdKaNWt000036Uc/+pGWLFmirVu36tChQ3rhhRcG/18yAPHn0uRxxggAAI7LKIw0NDRo5cqVOnXqlPx+v2bNmqVdu3bp5ptvliTV1tYqJydZ0BcuXKgtW7bokUce0cMPP6ypU6dq+/btmjlz5uD+KwYolBgZIYwAAOC0jMLIv/7rv/b6/t69ey+6dscdd+iOO+7IqFFOi2/tZTcNAADOo/pKCjNNAwCANVRfpU7TsIAVAACnEUaUurWX7gAAwGlUX6XspmHNCAAAjqP6KjlNQxgBAMB5VF8lF7AyTQMAgPOovkpu7eWcEQAAnEf1Veo0DbtpAABwGmFEyQWsTNMAAOA8qq+SW3tZwAoAgPOovkoZGSGMAADgOKqvGBkBAMAmqq+kUHw3DWtGAABwHNVXjIwAAGAT1Vepx8GztRcAAKcRRpQcGfEyTQMAgOOovmKaBgAAm6i+koI8tRcAAGuovkqOjLCbBgAA51F9lXxQHiMjAAA4j+qr1GfTsJsGAACnEUaUMk2Tm2u5JQAAZB/CiKRQhHNGAACwhTCilEPPWMAKAIDjqL5KnaahOwAAcBrVV8ndNGztBQDAeVRfSWEOPQMAwBqqr6QgC1gBALAm68OIMYYTWAEAsCjrq28kamQ6l4ywgBUAAAuyvvrGzxiRWDMCAIANWV99wx0m8T3TNAAAOC/rq2/qyIg7hwWsAAA4jTCScuCZy0UYAQDAaVkfRpJnjBBEAACwgTDCtl4AAKzK+goc5PRVAACsyvoKHI4QRgAAsCnrK3D8IXlepmkAALAi6yswIyMAANiV9RU4FF8z4mY3DQAANhBGUs4ZAQAAzsv6Csw0DQAAdmV9BY5P03DOCAAAdmR9BQ4zTQMAgFVZX4FDsa29TNMAAGBH1lfg5G6arO8KAACsyPoKnFzAytZeAABsIIzERkY4gRUAADuyvgKH2NoLAIBVWV+BCSMAANiV9RU43NG5m4ZzRgAAsCPrKzAnsAIAYFfWV+DECazspgEAwIqsDyOJE1iZpgEAwIqsr8AsYAUAwK6sr8CJE1gJIwAAWJH1FZgH5QEAYFfWV+BwhK29AADYlPUVmGkaAADsyvoKHOJBeQAAWJX1YYStvQAA2JX1FTh56FnWdwUAAFZkfQVOHAfPyAgAAFZkfQVO7KZhZAQAACuyvgJzAisAAHZlVIGrq6t13XXXqbi4WGPHjtWyZct0/PjxXn9n8+bNcrlcaa/8/PwBNXowJdaMuNlNAwCADRmFkTfeeEOrV6/WW2+9pd27dyscDuvrX/+6Wltbe/09n8+nU6dOJV4nTpwYUKMHU/IE1lzLLQEAIDu5M7n5P//zP9N+3rx5s8aOHavDhw/rq1/9ao+/53K5VFFR0b8WDrHkAlZGRgAAsGFACyWampokSaNHj+71vpaWFk2aNEmVlZVaunSpjh071uv9wWBQgUAg7TUUolGTWMDKmhEAAOzodwWORqNau3atbrzxRs2cObPH+6ZNm6ZNmzbptdde00svvaRoNKqFCxfqs88+6/F3qqur5ff7E6/Kysr+NrNX4Wg08T1hBAAAO1zGGNOfX7z//vu1c+dOvfnmm5owYUKffy8cDuvqq6/W8uXL9fjjj3d7TzAYVDAYTPwcCARUWVmppqYm+Xy+/jS3Wy3BDs38wS5J0u8fv1X5eawbAQBgsAQCAfn9/kvW74zWjMQ98MAD2rFjh/bt25dREJGkvLw8zZkzRx999FGP93i9Xnm93v40LSPxnTQSIyMAANiSUQU2xuiBBx7Qtm3b9Prrr2vy5MkZf2AkEtHRo0d15ZVXZvy7gy2+eDXHJeXmsIAVAAAbMhoZWb16tbZs2aLXXntNxcXFqqurkyT5/X4VFBRIklauXKnx48erurpakvTYY49pwYIFmjJlihobG7VhwwadOHFC99xzzyD/UzKXPGOEUREAAGzJKIxs3LhRkvS1r30t7fqLL76oO++8U5JUW1urnJxkcT9//rzuvfde1dXVqbS0VPPmzdP+/fs1Y8aMgbV8EHD6KgAA9mUURvqy1nXv3r1pP//4xz/Wj3/844wa5ZTkgWeEEQAAbMnqKhzuiD0kj2kaAACsyeoqzDQNAAD2ZXUVji9gzctlJw0AALZkdRhJrBlxc9gZAAC2EEYkeRgZAQDAmqwOI8lpmqzuBgAArMrqKswCVgAA7MvqKhyOsLUXAADbsroKM00DAIB9WV2Fk7tpWMAKAIAthBFxHDwAADZldRUOMk0DAIB1WV2F4yMjeSxgBQDAmqyuwkzTAABgX1ZXYbb2AgBgX1ZXYR6UBwCAfdkdRjiBFQAA67K6Coc74ueMZHU3AABgVVZX4RALWAEAsC6rq3CYaRoAAKzL6ioc6mA3DQAAtmV1FWYBKwAA9mV1FQ6ztRcAAOuyO4zERka8TNMAAGBNVldhpmkAALAvq6twiKf2AgBgXVZX4cSD8pimAQDAmqyuwvEH5TEyAgCAPVldhePTNJzACgCAPVldhRMnsLrZ2gsAgC1u2w2w6b8evEnBjqjKijy2mwIAQNbK6jBSWkgIAQDAtqyepgEAAPYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWuW03oC+MMZKkQCBguSUAAKCv4nU7Xsd7MizCSHNzsySpsrLScksAAECmmpub5ff7e3zfZS4VVy4D0WhUf/jDH1RcXCyXyzVofzcQCKiyslInT56Uz+cbtL+Li9HXzqGvnUNfO4v+ds5g9bUxRs3NzRo3bpxycnpeGTIsRkZycnI0YcKEIfv7Pp+P/7AdQl87h752Dn3tLPrbOYPR172NiMSxgBUAAFhFGAEAAFZldRjxer36wQ9+IK/Xa7spIx597Rz62jn0tbPob+c43dfDYgErAAAYubJ6ZAQAANhHGAEAAFYRRgAAgFWEEQAAYBVhBAAAWJXVYeQnP/mJrrrqKuXn5+uGG27Q22+/bbtJw1p1dbWuu+46FRcXa+zYsVq2bJmOHz+edk97e7tWr16tsrIyFRUV6c/+7M9UX19vqcUjx/r16+VyubR27drENfp6cH3++ef68z//c5WVlamgoEDXXnutDh06lHjfGKO/+7u/05VXXqmCggItXrxYH374ocUWD0+RSESPPvqoJk+erIKCAn3xi1/U448/nvagNfq6f/bt26dvfOMbGjdunFwul7Zv3572fl/69dy5c1qxYoV8Pp9KSkp09913q6WlZeCNM1lq69atxuPxmE2bNpljx46Ze++915SUlJj6+nrbTRu2brnlFvPiiy+a9957z7z77rvm9ttvNxMnTjQtLS2Je+677z5TWVlp9uzZYw4dOmQWLFhgFi5caLHVw9/bb79trrrqKjNr1iyzZs2axHX6evCcO3fOTJo0ydx5553m4MGD5uOPPza7du0yH330UeKe9evXG7/fb7Zv325++9vfmm9+85tm8uTJpq2tzWLLh58nn3zSlJWVmR07dphPPvnEvPLKK6aoqMg888wziXvo6/751a9+Zb7//e+bV1991Ugy27ZtS3u/L/166623mi9/+cvmrbfeMr/5zW/MlClTzPLlywfctqwNI9dff71ZvXp14udIJGLGjRtnqqurLbZqZGloaDCSzBtvvGGMMaaxsdHk5eWZV155JXHPf//3fxtJ5sCBA7aaOaw1NzebqVOnmt27d5ubbropEUbo68H10EMPma985Ss9vh+NRk1FRYXZsGFD4lpjY6Pxer3m3//9351o4oixZMkS8xd/8Rdp1/70T//UrFixwhhDXw+WrmGkL/36/vvvG0mmpqYmcc/OnTuNy+Uyn3/++YDak5XTNKFQSIcPH9bixYsT13JycrR48WIdOHDAYstGlqamJknS6NGjJUmHDx9WOBxO6/fp06dr4sSJ9Hs/rV69WkuWLEnrU4m+Hmy//OUvNX/+fN1xxx0aO3as5syZo5/97GeJ9z/55BPV1dWl9bff79cNN9xAf2do4cKF2rNnjz744ANJ0m9/+1u9+eabuu222yTR10OlL/164MABlZSUaP78+Yl7Fi9erJycHB08eHBAnz8snto72M6cOaNIJKLy8vK06+Xl5fr9739vqVUjSzQa1dq1a3XjjTdq5syZkqS6ujp5PB6VlJSk3VteXq66ujoLrRzetm7dqnfeeUc1NTUXvUdfD66PP/5YGzdu1IMPPqiHH35YNTU1+uu//mt5PB6tWrUq0afd/T+F/s7M9773PQUCAU2fPl25ubmKRCJ68skntWLFCkmir4dIX/q1rq5OY8eOTXvf7XZr9OjRA+77rAwjGHqrV6/We++9pzfffNN2U0akkydPas2aNdq9e7fy8/NtN2fEi0ajmj9/vp566ilJ0pw5c/Tee+/pueee06pVqyy3bmT5+c9/rpdffllbtmzRNddco3fffVdr167VuHHj6OsRLCunacaMGaPc3NyLdhbU19eroqLCUqtGjgceeEA7duzQr3/9a02YMCFxvaKiQqFQSI2NjWn30++ZO3z4sBoaGjR37ly53W653W698cYb+qd/+ie53W6Vl5fT14Poyiuv1IwZM9KuXX311aqtrZWkRJ/y/5SB+9u//Vt973vf07e//W1de+21+s53vqO/+Zu/UXV1tST6eqj0pV8rKirU0NCQ9n5HR4fOnTs34L7PyjDi8Xg0b9487dmzJ3EtGo1qz549qqqqstiy4c0YowceeEDbtm3T66+/rsmTJ6e9P2/ePOXl5aX1+/Hjx1VbW0u/Z2jRokU6evSo3n333cRr/vz5WrFiReJ7+nrw3HjjjRdtU//ggw80adIkSdLkyZNVUVGR1t+BQEAHDx6kvzN04cIF5eSkl6bc3FxFo1FJ9PVQ6Uu/VlVVqbGxUYcPH07c8/rrrysajeqGG24YWAMGtPx1GNu6davxer1m8+bN5v333zd/+Zd/aUpKSkxdXZ3tpg1b999/v/H7/Wbv3r3m1KlTideFCxcS99x3331m4sSJ5vXXXzeHDh0yVVVVpqqqymKrR47U3TTG0NeD6e233zZut9s8+eST5sMPPzQvv/yyGTVqlHnppZcS96xfv96UlJSY1157zfzud78zS5cuZbtpP6xatcqMHz8+sbX31VdfNWPGjDHf/e53E/fQ1/3T3Nxsjhw5Yo4cOWIkmX/8x380R44cMSdOnDDG9K1fb731VjNnzhxz8OBB8+abb5qpU6eytXeg/vmf/9lMnDjReDwec/3115u33nrLdpOGNUndvl588cXEPW1tbeav/uqvTGlpqRk1apT5kz/5E3Pq1Cl7jR5BuoYR+npw/cd//IeZOXOm8Xq9Zvr06eaFF15Iez8ajZpHH33UlJeXG6/XaxYtWmSOHz9uqbXDVyAQMGvWrDETJ040+fn55gtf+IL5/ve/b4LBYOIe+rp/fv3rX3f7/+hVq1YZY/rWr2fPnjXLly83RUVFxufzmbvuuss0NzcPuG0uY1KOtQMAAHBYVq4ZAQAAlw/CCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKz6/1oIUUOJEsipAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(norm_history.mean(dim=-1).detach())\n",
    "plt.ylim(2.1, 4.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
