{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs01/berens/user/jdoehl/GitHub/sssl/.venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['model.certainty_head.0.weight', 'model.certainty_head.0.bias', 'model.certainty_head.2.weight', 'model.certainty_head.2.bias']\n"
     ]
    }
   ],
   "source": [
    "from lightning_simclr import SimCLR\n",
    "ckpt = \"logs/No_schedule-_No_decay-1000/l2/no_decay-no_momentum/checkpoint/epoch=999.ckpt\"\n",
    "mod = SimCLR.load_from_checkpoint(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimCLR(\n",
       "  (model): ResNet18withProjector(\n",
       "    (backbone): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): Identity()\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Identity()\n",
       "    )\n",
       "    (projector): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    )\n",
       "    (certainty_head): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"base_lr\":                  0.06\n",
      "\"batch_size\":               1024\n",
      "\"cut\":                      1\n",
      "\"dataset_name\":             cifar10\n",
      "\"n_epochs\":                 1000\n",
      "\"nestrov\":                  False\n",
      "\"norm_function_str\":        l2\n",
      "\"norm_reguralization_clip\": 0.002\n",
      "\"power\":                    0\n",
      "\"seed\":                     1145799988\n",
      "\"warmup_epochs\":            1\n",
      "\"weight_decay\":             0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "\n",
      "  | Name  | Type                  | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model | ResNet18withProjector | 11.8 M | train\n",
      "--------------------------------------------------------\n",
      "11.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.8 M    Total params\n",
      "47.301    Total estimated model params size (MB)\n",
      "73        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/gpfs01/berens/user/jdoehl/GitHub/sssl/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a933ef8dced4bd2bcb16b494d0b235d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "from lightning_simclr import SimCLR\n",
    "from data_utils import get_augmented_dataloader\n",
    "import lightning as L\n",
    "\n",
    "dataset_name = \"cifar10\"\n",
    "\n",
    "mod = SimCLR(dataset_name=dataset_name, n_epochs=1000, norm_reguralization_clip=0.00)\n",
    "print(mod.hparams)\n",
    "trainer = L.Trainer(max_epochs=20)\n",
    "data_loader = get_augmented_dataloader(mod.dataset_name)\n",
    "trainer.fit(mod, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logs/Unbalanced-1000/exp_map/version_2/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/exp_map/version_0/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/exp_map/version_1/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/mono/version_2/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/mono/version_0/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/mono/version_1/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_2/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_3/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_5/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_7/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_4/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_0/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_6/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_1/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/l2/version_8/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/stereo/version_3/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/stereo/version_0/checkpoint/epoch=999.ckpt',\n",
       " 'logs/Unbalanced-1000/stereo/version_1/checkpoint/epoch=999.ckpt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "name = \"epoch=999.ckpt\"\n",
    "log_dir = \"logs/Unbalanced-1000\"\n",
    "fully_trained_ckpt_paths = []\n",
    "for (root,dirs,files) in os.walk(log_dir, topdown=True):\n",
    "    if name in files:\n",
    "        fully_trained_ckpt_paths.append(root+\"/\"+name)\n",
    "fully_trained_ckpt_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2\n",
      "cut: 1\n",
      "cifar10_unbalanced, knn acc: 0.5837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightning_simclr import SimCLR\n",
    "from data_utils import get_train_and_test_set, CIFAR10, CIFAR10_Unbalanced\n",
    "import os\n",
    "name = \"epoch=499.ckpt\"\n",
    "conidtion2 = \"version_5\"\n",
    "log_dir = \"logs/Unbalanced-1000\"\n",
    "specific_ckpts = []\n",
    "for (root,dirs,files) in os.walk(log_dir, topdown=True):\n",
    "    if name in files:\n",
    "        if conidtion2 in root:\n",
    "            specific_ckpts.append(root+\"/\"+name)\n",
    "print(specific_ckpts)\n",
    "\n",
    "def get_accs_for_ckpts(\n",
    "        ckpt_paths,\n",
    "        knn_datasets = {\"cifar10_unbalanced\": CIFAR10_Unbalanced}\n",
    "        ):\n",
    "    acc_dict = {key:{} for key in knn_datasets.keys()}\n",
    "    for ckpt in ckpt_paths:\n",
    "        mod = SimCLR.load_from_checkpoint(ckpt, strict=False)\n",
    "        print(mod.norm_function_str)\n",
    "        print(\"cut:\", mod.hparams[\"cut\"])\n",
    "        for dataset_name, Dataset in knn_datasets.items():\n",
    "            train, test = get_train_and_test_set(Dataset)\n",
    "            acc = mod.get_knn_acc(train, test, n_neighbors=200)\n",
    "            acc_dict[dataset_name] = acc\n",
    "            print(f\"{dataset_name}, knn acc:\", acc)\n",
    "        print()\n",
    "    return acc_dict\n",
    "\n",
    "acc_dict = get_accs_for_ckpts(specific_ckpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_map\n",
      "cut: 1\n",
      "cifar10_unbalanced, knn acc: 0.5903\n",
      "\n",
      "mono\n",
      "cut: 1\n",
      "cifar10_unbalanced, knn acc: 0.6137\n",
      "\n",
      "l2\n",
      "cut: 3\n",
      "cifar10_unbalanced, knn acc: 0.6325\n",
      "\n",
      "stereo\n",
      "cut: 1\n",
      "cifar10_unbalanced, knn acc: 0.6132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightning_simclr import SimCLR\n",
    "from data_utils import get_train_and_test_set, CIFAR10, CIFAR10_Unbalanced\n",
    "import os\n",
    "name = \"epoch=999.ckpt\"\n",
    "conidtion2 = \"version_0\"\n",
    "log_dir = \"logs/Unbalanced-1000\"\n",
    "specific_ckpts = []\n",
    "for (root,dirs,files) in os.walk(log_dir, topdown=True):\n",
    "    if name in files:\n",
    "        if conidtion2 in root:\n",
    "            specific_ckpts.append(root+\"/\"+name)\n",
    "\n",
    "\n",
    "def get_accs_for_ckpts(\n",
    "        ckpt_paths,\n",
    "        knn_datasets = {\"cifar10_unbalanced\": CIFAR10_Unbalanced}\n",
    "        ):\n",
    "    acc_dict = {key:{} for key in knn_datasets.keys()}\n",
    "    for ckpt in ckpt_paths:\n",
    "        mod = SimCLR.load_from_checkpoint(ckpt, strict=False)\n",
    "        print(mod.norm_function_str)\n",
    "        print(\"cut:\", mod.hparams[\"cut\"])\n",
    "        for dataset_name, Dataset in knn_datasets.items():\n",
    "            train, test = get_train_and_test_set(Dataset)\n",
    "            acc = mod.get_knn_acc(train, test, n_neighbors=200)\n",
    "            acc_dict[dataset_name] = acc\n",
    "            print(f\"{dataset_name}, knn acc:\", acc)\n",
    "        print()\n",
    "    return acc_dict\n",
    "\n",
    "acc_dict = get_accs_for_ckpts(specific_ckpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_var(arr):\n",
    "    print(\"mean:\", np.mean(arr).item()*100, \"std:\", f\"{np.std(arr).item()*100:.5f}\")\n",
    "\n",
    "# Unbalanced Cifar10\n",
    "acc_500 = dict(\n",
    "    l2 = [0.656, 0.662, 0.653],\n",
    "    cut3 = [0.653, 0.637, 0.649],\n",
    "    exp_map = [0.613, 0.621, 0.624],\n",
    "    stereo = [0.631, 0.627, 0.629],\n",
    "    mono = [0.639, 0.642, 0.637],\n",
    "    # grad_scale = []\n",
    ")\n",
    "\n",
    "for key, value in acc_500.items():\n",
    "    print(key)\n",
    "    mean_var(value)\n",
    "print()\n",
    "\n",
    "acc_1k = dict(\n",
    "    l2 = [0.689, 0.691, 0.693],\n",
    "    cut3 = [0.69, 0.697, 0.695],\n",
    "    exp_map = [0.648, 0.641, 0.639],\n",
    "    stereo = [0.643, 0.642, 0.646],\n",
    "    mono = [0.653, 0.656, 0.665],\n",
    "    # grad_scale = []\n",
    ")\n",
    "\n",
    "for key, value in acc_1k.items():\n",
    "    print(key)\n",
    "    mean_var(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced Cifar10\n",
    "A Cifar10 version where datapoints where systematically removed to make it unbalanced.  \n",
    "For each variant three seeds were tested. The numbers shown in the table are: **Mean $\\pm$ Std.**\n",
    "\n",
    "| Epoch (out of 1000)     | L2   (Baseline)             | Cut                  | Exp Map              | Stereo            | Mono                 |\n",
    "|------------------|-------------------|----------------------|----------------------|-------------------|----------------------|\n",
    "| 500| **65.7 ± 0.37** | 64.3333 ± 0.67 | 61.9333 ± 0.46 | 62.9 ± 0.16 | 63.9333 ± 0.20 |  \n",
    "| 1000 | 69.1 ± 0.16 | **69.4 ± 0.29** | 64.2666 ± 0.38 | 64.3666 ± 0.16 | 65.8 ± 0.5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_var(arr):\n",
    "    print(\"mean:\", np.mean(arr).item()*100, \"std:\", f\"{np.std(arr).item()*100:.5f}\")\n",
    "\n",
    "# Cifar10\n",
    "acc_500 = dict(\n",
    "    l2 = [0.87, 0.871, 0.869],\n",
    "    cut3 = [0.868, 0.869, 0.865],\n",
    "    exp_map = [0.854, 0.854, 0.855],\n",
    "    stereo = [0.864, 0.862, 0.864],\n",
    "    mono = [0.865, 0.866, 0.867],\n",
    "    # grad_scale = []\n",
    ")\n",
    "print(\"epoch 500:\")\n",
    "for key, value in acc_500.items():\n",
    "    print(key)\n",
    "    mean_var(value)\n",
    "print()\n",
    "\n",
    "acc_1k = dict(\n",
    "    l2 = [0.9018, 0.9047, 0.9026],\n",
    "    cut3 = [0.901, 0.9045, 0.9036],\n",
    "    exp_map = [0.8954, 0.8989, 0.8956],\n",
    "    stereo = [0.9017, 0.9023, 0.902],\n",
    "    mono = [0.9005, 0.9057, 0.9008],\n",
    "    # grad_scale = []\n",
    ")\n",
    "print(\"epoch 1000:\")\n",
    "for key, value in acc_1k.items():\n",
    "    print(key)\n",
    "    mean_var(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar10\n",
    "For each variant three seeds were tested. The numbers shown in the table are: **Mean ± Std.**\n",
    "\n",
    "| Epoch (out of 1000)   | L2 (Baseline)  | Cut          | Exp Map          | Stereo        | Mono              |\n",
    "|------------------|----------------|--------------|------------------|---------------|-------------------|\n",
    "| 500  | **87 ± 0.08** | 86.73 ± 0.16 | 85.43 ± 0.04  | 86.33 ± 0.09 | 86.6 ± 0.08 | \n",
    "| 1000 | **90.30 ± 0.12** |**90.30 ± 0.14** | 89.66 ± 0.16  | 90.2 ± 0.24 | 90.23 ± 0.23 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
